{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSC 421 - Assignment 4 - Classifying handwritten digits using Numpy and Tensorflow.\n",
    "Devroop Banerjee\n",
    "V00837868\n",
    "--\n",
    "For this assignment, you should complete the exercises in this notebook. It is similar to the notebook posted for binary logistic regression. \n",
    "\n",
    "Look for requests containing the text **\"your code\"**. E.g. \"put your code here\", \"replace None by your code\", etc.\n",
    "If there is no such request in a cell, just run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's load the dataset of handwritten digits from a Python pickle file. \n",
    "# For information on pickle files, see: https://pythontips.com/2013/08/02/what-is-pickle-in-python\n",
    "# The pickle file contains 55,000 training images and their labels as well as\n",
    "# 10,000 test images and their labels.\n",
    "\n",
    "fileObject = open('E:\\study\\CSC421\\A4\\mnist_nonbin_classification.pickle','rb')  \n",
    "X,Y,X_test,Y_test = pickle.load(fileObject)\n",
    "fileObject.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADfhJREFUeJzt3X+o1XWex/HXO3fsh4ooXn/Q6N5J\nLstUtI4cLCuWlmhqlgGbaGoUxGDQiAl2aIQtESaCjcuyNiu0DDmbjIaTM6SOErFrxZIJ0+DJanKy\nXSvujqbp1YLJ/EO8vveP+3W42f1+zvF8v+d8z73v5wPinPN9f3+8+ebrfs853+/5fszdBSCey6pu\nAEA1CD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaD+qpMbmzFjhvf29nZyk0AoAwMDOnnypDUz\nb6Hwm9ldktZLmiDpP9y9PzV/b2+v6vV6kU0CSKjVak3P2/LbfjObIOnfJX1H0rWSlprZta2uD0Bn\nFfnMv0jSB+7+kbuflbRV0pJy2gLQbkXCf7WkwyNeH8mmfYmZrTKzupnVBwcHC2wOQJmKhH+0LxW+\n8vtgd9/g7jV3r/X09BTYHIAyFQn/EUlzR7z+uqSjxdoB0ClFwr9PUp+ZfcPMJkr6gaRd5bQFoN1a\nPtXn7ufM7GFJ/6XhU30b3f2PpXUGoK0Kned395ckvVRSLwA6iMt7gaAIPxAU4QeCIvxAUIQfCIrw\nA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK\n8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCKrQKL1mNiDpc0lDks65e62MpgC0X6HwZ/7e3U+W\nsB4AHcTbfiCoouF3SbvN7E0zW1VGQwA6o+jb/lvc/aiZzZT0spm97+57Rs6Q/VFYJUnz5s0ruDkA\nZSl05Hf3o9njCUk7JC0aZZ4N7l5z91pPT0+RzQEoUcvhN7NJZjblwnNJ35Z0oKzGALRXkbf9syTt\nMLML6/mVu/9nKV0BaLuWw+/uH0n62xJ7AdBBnOoDgiL8QFCEHwiK8ANBEX4gKMIPBFXGr/pQsVde\neSW3ll2HkWvatGnJ+oED6eu2Fi9enKz39fUl66gOR34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCGrc\nnOffs2dPsv7GG28k6+vWrSuznY46depUy8tOmDAhWT979myyftVVVyXrkydPzq3deuutyWWfe+65\nQttGGkd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwhqTJ3n7+/vz62tXbs2uezQ0FDZ7YwLRffLmTNn\nWq5v3749uWyjexFs2rQpWZ80aVKyHh1HfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IquF5fjPbKOm7\nkk64+/XZtOmSfi2pV9KApPvc/bP2tTnsmWeeya01Ol990003JetTpkxpqacy3H777cn6Pffc06FO\nLt3u3buT9fXr1+fWDh06lFx227ZtLfV0webNm3Nr3AuguSP/LyXdddG0RyW96u59kl7NXgMYQxqG\n3933SPr0oslLJF24vGqTpLtL7gtAm7X6mX+Wux+TpOxxZnktAeiEtn/hZ2arzKxuZvXBwcF2bw5A\nk1oN/3EzmyNJ2eOJvBndfYO719y91tPT0+LmAJSt1fDvkrQie75C0s5y2gHQKQ3Db2bPS/qdpL8x\nsyNm9kNJ/ZLuMLNDku7IXgMYQ8zdO7axWq3m9Xq95eVPnjyZW/vwww+Tyy5YsCBZv/zyy1vqCWmf\nfZZ/+Uej6xveeuutQtvesmVLbm3ZsmWF1t2tarWa6vV6+kYIGa7wA4Ii/EBQhB8IivADQRF+ICjC\nDwQ1pk71YXxpNGz64sWLC61/1qxZubVPPvmk0Lq7Faf6ADRE+IGgCD8QFOEHgiL8QFCEHwiK8ANB\nEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0E1HKIbKGLnzvzxXPbu3dvWbX/xxRe5tcOHDyeX\nnTt3btntdB2O/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVMPz/Ga2UdJ3JZ1w9+uzaY9LWilpMJtt\njbu/1K4mkXb69Onc2o4dO5LLrl27tux2viR1Pr3dY0ak9ssNN9yQXDY1tPh40cyR/5eS7hpl+s/c\nfUH2H8EHxpiG4Xf3PZI+7UAvADqoyGf+h83sD2a20cymldYRgI5oNfw/lzRf0gJJxySty5vRzFaZ\nWd3M6oODg3mzAeiwlsLv7sfdfcjdz0v6haRFiXk3uHvN3Ws9PT2t9gmgZC2F38zmjHj5PUkHymkH\nQKc0c6rveUm3SZphZkck/VTSbWa2QJJLGpD0YBt7BNAGDcPv7ktHmfxsG3oJ67333kvW9+3bl6z3\n9/fn1t5///2WehrvVq9eXXULleMKPyAowg8ERfiBoAg/EBThB4Ii/EBQ3Lq7BKdOnUrWH3rooWT9\nhRdeSNbb+dPX+fPnJ+uzZ88utP6nn346tzZx4sTkssuWLUvW33nnnZZ6kqR58+a1vOx4wZEfCIrw\nA0ERfiAowg8ERfiBoAg/EBThB4LiPH+Ttm7dmlt74oknkssePHgwWZ8yZUqyPn369GT9ySefzK01\nGmq60S2sp06dmqy3U9E7P6V6v/POOwutezzgyA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQXGev0mv\nvfZabq3RefwHHnggWV+zZk2y3tfXl6yPVR9//HGy3uiW5o1cccUVubWZM2cWWvd4wJEfCIrwA0ER\nfiAowg8ERfiBoAg/EBThB4JqeJ7fzOZK2ixptqTzkja4+3ozmy7p15J6JQ1Ius/dP2tfq9V66qmn\ncmsLFy5MLrty5cqy2xkXDh8+nKwfPXq00PrvvffeQsuPd80c+c9J+om7f1PSTZJ+ZGbXSnpU0qvu\n3ifp1ew1gDGiYfjd/Zi778+efy7poKSrJS2RtCmbbZOku9vVJIDyXdJnfjPrlfQtSb+XNMvdj0nD\nfyAkcb0kMIY0HX4zmyxpm6Qfu/ufL2G5VWZWN7P64OBgKz0CaIOmwm9mX9Nw8Le4+/Zs8nEzm5PV\n50g6Mdqy7r7B3WvuXit6Q0YA5WkYfjMzSc9KOujuI7/y3iVpRfZ8haSd5bcHoF2a+UnvLZKWS3rX\nzN7Opq2R1C/pN2b2Q0l/kvT99rTYHa688srcGqfyWpP6mXQzGt3S/JFHHim0/vGuYfjdfa8kyynf\nXm47ADqFK/yAoAg/EBThB4Ii/EBQhB8IivADQXHrbrTVjTfemFvbv39/oXXff//9yfo111xTaP3j\nHUd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8/xoq9Tw5efOnUsuO23atGR99erVLfWEYRz5gaAI\nPxAU4QeCIvxAUIQfCIrwA0ERfiAozvOjkNdffz1ZP3PmTG5t6tSpyWVffPHFZJ3f6xfDkR8IivAD\nQRF+ICjCDwRF+IGgCD8QFOEHgmp4nt/M5kraLGm2pPOSNrj7ejN7XNJKSYPZrGvc/aV2NYpqDA0N\nJeuPPfZYsj5x4sTc2sqVK5PL3nzzzck6imnmIp9zkn7i7vvNbIqkN83s5az2M3f/1/a1B6BdGobf\n3Y9JOpY9/9zMDkq6ut2NAWivS/rMb2a9kr4l6ffZpIfN7A9mttHMRr3nkpmtMrO6mdUHBwdHmwVA\nBZoOv5lNlrRN0o/d/c+Sfi5pvqQFGn5nsG605dx9g7vX3L3W09NTQssAytBU+M3saxoO/hZ33y5J\n7n7c3Yfc/bykX0ha1L42AZStYfjNzCQ9K+mguz81YvqcEbN9T9KB8tsD0C7NfNt/i6Tlkt41s7ez\naWskLTWzBZJc0oCkB9vSISo1/Lc/34MPpv+3L1y4MLd23XXXtdQTytHMt/17JY32L4Bz+sAYxhV+\nQFCEHwiK8ANBEX4gKMIPBEX4gaC4dTeSLrssfXxYvnx5hzpB2TjyA0ERfiAowg8ERfiBoAg/EBTh\nB4Ii/EBQ5u6d25jZoKT/GzFphqSTHWvg0nRrb93al0RvrSqzt79296bul9fR8H9l42Z1d69V1kBC\nt/bWrX1J9NaqqnrjbT8QFOEHgqo6/Bsq3n5Kt/bWrX1J9NaqSnqr9DM/gOpUfeQHUJFKwm9md5nZ\n/5jZB2b2aBU95DGzATN718zeNrN6xb1sNLMTZnZgxLTpZvaymR3KHkcdJq2i3h43s4+zffe2mf1D\nRb3NNbP/NrODZvZHM/vHbHql+y7RVyX7reNv+81sgqT/lXSHpCOS9kla6u7vdbSRHGY2IKnm7pWf\nEzazv5N0WtJmd78+m/Yvkj519/7sD+c0d/+nLuntcUmnqx65ORtQZs7IkaUl3S3pAVW47xJ93acK\n9lsVR/5Fkj5w94/c/aykrZKWVNBH13P3PZI+vWjyEkmbsuebNPyPp+NyeusK7n7M3fdnzz+XdGFk\n6Ur3XaKvSlQR/qslHR7x+oi6a8hvl7TbzN40s1VVNzOKWdmw6ReGT59ZcT8XazhycyddNLJ01+y7\nVka8LlsV4R9t9J9uOuVwi7svlPQdST/K3t6iOU2N3Nwpo4ws3RVaHfG6bFWE/4ikuSNef13S0Qr6\nGJW7H80eT0jaoe4bffj4hUFSs8cTFffzF900cvNoI0urC/ZdN414XUX490nqM7NvmNlEST+QtKuC\nPr7CzCZlX8TIzCZJ+ra6b/ThXZJWZM9XSNpZYS9f0i0jN+eNLK2K9123jXhdyUU+2amMf5M0QdJG\nd//njjcxCjO7RsNHe2n4zsa/qrI3M3te0m0a/tXXcUk/lfRbSb+RNE/SnyR93907/sVbTm+3afit\n619Gbr7wGbvDvd0q6XVJ70o6n01eo+HP15Xtu0RfS1XBfuMKPyAorvADgiL8QFCEHwiK8ANBEX4g\nKMIPBEX4gaAIPxDU/wOQv/IG3GepCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b5a4863630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's view some images\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(X[0,:].reshape((28,28)), cmap='Greys')\n",
    "\n",
    "print(Y[0])\n",
    "print(Y_test[0])\n",
    "\n",
    "# Feel free to display other images by changing the index 0 above to some other index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape (55000, 784) (55000, 1)\n",
      "Test dataset shape (10000, 784) (10000, 1)\n",
      "Y = [[7]\n",
      " [3]\n",
      " [4]\n",
      " ..., \n",
      " [5]\n",
      " [6]\n",
      " [8]]\n"
     ]
    }
   ],
   "source": [
    "# We will reshape the Y arrays so that they are not rank 1 arrays but rank 2 arrays. \n",
    "# They should be rank 2 arrays.\n",
    "\n",
    "Y = Y.reshape((Y.shape[0],1))\n",
    "Y_test = Y_test.reshape((Y_test.shape[0],1))\n",
    "\n",
    "print(\"Train dataset shape\", X.shape, Y.shape)\n",
    "print(\"Test dataset shape\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(\"Y =\", Y)\n",
    "\n",
    "m   = X.shape[0] \n",
    "n_x = X.shape[1]\n",
    "\n",
    "C = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 - One Hot Encoding\n",
    "--\n",
    "\n",
    "Convert Y to \"one-hot\" encoding. E.g. if the original Y is \n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    5 \\\\\n",
    "    9  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "the new Y should be\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "    0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\ \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toward this goal, let's check what np.arange(C) produces\n",
    "np.arange(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [3],\n",
       "       [4],\n",
       "       ..., \n",
       "       [5],\n",
       "       [6],\n",
       "       [8]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see again what Y is\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcasted np.arange(C) = \n",
      " [[0 1 2 ..., 7 8 9]\n",
      " [0 1 2 ..., 7 8 9]\n",
      " [0 1 2 ..., 7 8 9]\n",
      " ..., \n",
      " [0 1 2 ..., 7 8 9]\n",
      " [0 1 2 ..., 7 8 9]\n",
      " [0 1 2 ..., 7 8 9]]\n",
      "broadcasted Y = \n",
      " [[7 7 7 ..., 7 7 7]\n",
      " [3 3 3 ..., 3 3 3]\n",
      " [4 4 4 ..., 4 4 4]\n",
      " ..., \n",
      " [5 5 5 ..., 5 5 5]\n",
      " [6 6 6 ..., 6 6 6]\n",
      " [8 8 8 ..., 8 8 8]]\n"
     ]
    }
   ],
   "source": [
    "# What would broadcasting these arrays together would look like? \n",
    "# Let's check.\n",
    "\n",
    "a,b = np.broadcast_arrays(np.arange(C), Y)\n",
    "\n",
    "print(\"broadcasted np.arange(C) = \\n\", a)\n",
    "print(\"broadcasted Y = \\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_new= [[False False False ...,  True False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False  True False]]\n",
      "Y= [[0 0 0 ..., 1 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "Y_new_test= [[False False False ...,  True False False]\n",
      " [False False  True ..., False False False]\n",
      " [False  True False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]]\n",
      "Y_test= [[0 0 0 ..., 1 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# If we compare np.arange(C) with Y using the equality sign ==, \n",
    "# the numpy broadcasting will do its magic to give us what we want. \n",
    "# Try it out. Then assign the result to a new variable Y_new. \n",
    "# Don't worry for the \"True\" and \"False\" values looking like strings. \n",
    "# They behave in fact like numbers, i.e. True is 1, False is 0.\n",
    "# Finally, assign Y_new to Y so that we have to deal with Y in rest of the notebook.\n",
    "# Cast the boolean values of Y_new to integer by calling Y_new.astype(int)\n",
    "\n",
    "# Put your code in place of None objects \n",
    "\n",
    "Y_new = np.arange(C)==Y\n",
    "print(\"Y_new=\", Y_new)\n",
    "Y=Y_new.astype(int)\n",
    "print(\"Y=\",Y)\n",
    "\n",
    "Y_new_test = np.arange(C) ==Y_test\n",
    "print(\"Y_new_test=\", Y_new_test)\n",
    "Y_test = Y_new_test.astype(int)\n",
    "print(\"Y_test=\",Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "Y_new= [[False False False ...,  True False False]\n",
    " [False False False ..., False False False]\n",
    " [False False False ..., False False False]\n",
    " ..., \n",
    " [False False False ..., False False False]\n",
    " [False False False ..., False False False]\n",
    " [False False False ..., False  True False]]\n",
    "Y= [[0 0 0 ..., 1 0 0]\n",
    " [0 0 0 ..., 0 0 0]\n",
    " [0 0 0 ..., 0 0 0]\n",
    " ..., \n",
    " [0 0 0 ..., 0 0 0]\n",
    " [0 0 0 ..., 0 0 0]\n",
    " [0 0 0 ..., 0 1 0]]\n",
    "Y_new_test= [[False False False ...,  True False False]\n",
    " [False False  True ..., False False False]\n",
    " [False  True False ..., False False False]\n",
    " ..., \n",
    " [False False False ..., False False False]\n",
    " [False False False ..., False False False]\n",
    " [False False False ..., False False False]]\n",
    "Y_test= [[0 0 0 ..., 1 0 0]\n",
    " [0 0 1 ..., 0 0 0]\n",
    " [0 1 0 ..., 0 0 0]\n",
    " ..., \n",
    " [0 0 0 ..., 0 0 0]\n",
    " [0 0 0 ..., 0 0 0]\n",
    " [0 0 0 ..., 0 0 0]]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 - The Softmax Function\n",
    "--\n",
    "(Adapted from Andrew Ng's exercise in Coursera, deeplearning.ai)\n",
    "\n",
    "Implement a softmax function using numpy. Softmax is a normalizing function used when the algorithm needs to classify two or more classes. \n",
    "\n",
    "**Instructions**:\n",
    "- for $x \\in \\mathbb{R}^{1\\times n}$ \n",
    "$$\\mbox{softmax}(x) = \\mbox{softmax}\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $$ \n",
    "\n",
    "- for a matrix $x \\in \\mathbb{R}^{m \\times n}$  \n",
    "$$\\mbox{softmax}(x) = \\mbox{softmax}\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    \\mbox{softmax}\\text{(first row of x)}  \\\\\n",
    "    \\mbox{softmax}\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    \\mbox{softmax}\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[  6.74508059e-02   1.83350300e-01   4.98397788e-01   6.74508059e-02\n",
      "    1.83350300e-01]\n",
      " [  9.81452586e-01   1.79759312e-02   3.29240664e-04   1.21120871e-04\n",
      "    1.21120871e-04]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    # Create an array x_exp by applying np.exp() element-wise to x. \n",
    "    # Put your code here (one line)\n",
    "    x_exp=np.exp(x)\n",
    "    # Create an array x_sum that contains the sum of each row of x_exp. \n",
    "    # Use np.sum(..., axis = 1, keepdims = True).\n",
    "    # Put your code here (one line)\n",
    "    x_sum=np.sum(x_exp, axis=1, keepdims= True)\n",
    "    #print(x_sum)\n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    # Return this array.\n",
    "    # Replace None with your code\n",
    "    return x_exp/x_sum\n",
    "\n",
    "\n",
    "# Let's test\n",
    "x = np.array([\n",
    "    [1, 2, 3, 1, 2],\n",
    "    [9, 5, 1, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "softmax(x) = [[  6.74508059e-02   1.83350300e-01   4.98397788e-01   6.74508059e-02\n",
    "    1.83350300e-01]\n",
    " [  9.81452586e-01   1.79759312e-02   3.29240664e-04   1.21120871e-04\n",
    "    1.21120871e-04]]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3 - Compute one semi-vectorized iteration for softmax\n",
    "--\n",
    "Perform one semi-vectorized iteration of gradient descent for the softmax classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [[0 0 0 0 0 0 0 1 0 0]]\n",
      "z [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "a [[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]]\n",
      "J 2.30258509299\n",
      "dz [[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1 -0.9  0.1  0.1]]\n",
      "dw [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "db [[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1 -0.9  0.1  0.1]]\n"
     ]
    }
   ],
   "source": [
    "# First do this for only one training example (the first one, i=0).\n",
    "# Print out everything you compute, e.g. print(\"z\", z), print(\"a\", a), etc. \n",
    "\n",
    "J = 0\n",
    "w = np.zeros((n_x,C))\n",
    "b = np.zeros((1,C))\n",
    "\n",
    "dw = np.zeros((n_x,C));\n",
    "db = np.zeros((1,C));\n",
    "\n",
    "i = 0;\n",
    "\n",
    "\n",
    "# Replace None objects by your code\n",
    "\n",
    "x = X[i:i+1, : ]  #x is [1,784]\n",
    "y = Y[i:i+1, : ]\n",
    "print(\"y\", y)\n",
    "\n",
    "z = x @ w + b\n",
    "print(\"z\", z)\n",
    "a = softmax(z)\n",
    "print(\"a\", a)\n",
    "J += np.sum(-y * np.log(a))\n",
    "print(\"J\", J)\n",
    "\n",
    "dz = a-y\n",
    "print(\"dz\", dz)\n",
    "dw += x.T * dz\n",
    "print(\"dw\", dw)\n",
    "db += dz\n",
    "print(\"db\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "y [[0 0 0 0 0 0 0 1 0 0]]\n",
    "z [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
    "a [[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]]\n",
    "J 2.30258509299\n",
    "dz [[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1 -0.9  0.1  0.1]]\n",
    "dw [[ 0.  0.  0. ...,  0.  0.  0.]\n",
    " [ 0.  0.  0. ...,  0.  0.  0.]\n",
    " [ 0.  0.  0. ...,  0.  0.  0.]\n",
    " ..., \n",
    " [ 0.  0.  0. ...,  0.  0.  0.]\n",
    " [ 0.  0.  0. ...,  0.  0.  0.]\n",
    " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
    "db [[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1 -0.9  0.1  0.1]]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J 2.302585093\n",
      "Time needed:  3.3213651180267334\n"
     ]
    }
   ],
   "source": [
    "#one iteration, semi-vectorized\n",
    "\n",
    "J = 0\n",
    "w = np.zeros((n_x,C))\n",
    "b = np.zeros((1,C))\n",
    "\n",
    "dw = np.zeros((n_x,C));\n",
    "db = np.zeros((1,C));\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(m):\n",
    "    # Put your code here\n",
    "    x = X[i:i+1, : ]  #x is [1,784]\n",
    "    y = Y[i:i+1, : ]\n",
    "    #print(\"y\", y)\n",
    "\n",
    "    z = x @ w + b\n",
    "    #print(\"z\", z)\n",
    "    a = softmax(z)\n",
    "   # print(\"a\", a)\n",
    "    J += np.sum(-y * np.log(a))\n",
    "    #print(\"J\", J)\n",
    "\n",
    "    dz = a-y\n",
    "   # print(\"dz\", dz)\n",
    "    dw += x.T * dz\n",
    "    #print(\"dw\", dw)\n",
    "    db += dz\n",
    "    #print(\"db\", db)\n",
    "    \n",
    "    \n",
    "\n",
    "J /= m; dw /= m; db /= m\n",
    "\n",
    "w -= alpha*dw\n",
    "b -= alpha*db\n",
    "\n",
    "print(\"J\", J)\n",
    "print(\"Time needed: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "J =  2.302585093\n",
    "Time needed:  3.797287702560425\n",
    "</pre>\n",
    "\n",
    "Of course, your running time will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4 - Compute one fully-vectorized iteration for softmax\n",
    "--\n",
    "Perform one fully-vectorized iteration of gradient descent for the softmax classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J =  2.30258509299\n",
      "Time needed:  0.5166137218475342\n"
     ]
    }
   ],
   "source": [
    "#one iteration, fully vectorized, no for loop\n",
    "\n",
    "J = 0\n",
    "w = np.zeros((n_x,C))\n",
    "b = np.zeros((1,C))\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#Replace the None objects and alpha*0 by your code.\n",
    "\n",
    "# Convention: \n",
    "# Use capital letter when the variable is for the whole dataset of m train examples.\n",
    "\n",
    "# X is (55000,784), Y is (55000,10), w is (784,10), b is (1,10)  \n",
    "Z = X @ w + b               # Z is  (55000, 10)\n",
    "A = softmax(Z)              # A is  (55000, 10)\n",
    "J = -(1/m)*np.sum(Y * np.log(A))\n",
    "\n",
    "dZ = A - Y# dZ is (55000, 10)\n",
    "\n",
    "\n",
    "dw = (1/m) * X.T @ dZ                #dw is (784, 10) \n",
    "db = (1/m) * np.sum(dZ,axis=0, keepdims=True)\n",
    "\n",
    "w -= alpha*dw\n",
    "b -= alpha*db\n",
    "\n",
    "print(\"J = \", J)\n",
    "print(\"Time needed: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "J =  2.30258509299\n",
    "Time needed:  0.5495262145996094\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the time of the fully vectorized version is almost one order of magnitude smaller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5 - Compute several fully-vectorized iterations for softmax\n",
    "--\n",
    "Perform 100 fully-vectorized iterations of gradient descent for the softmax classification.\n",
    "Start with doing 10 iterations first, check the accuracy you achieve, then try for 100 iterations. \n",
    "Print out the cost after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.30258509299\n",
      "1 2.19706377599\n",
      "2 2.10085951391\n",
      "3 2.0120441413\n",
      "4 1.92974131203\n",
      "5 1.85342090129\n",
      "6 1.78266241239\n",
      "7 1.71707943589\n",
      "8 1.65629942394\n",
      "9 1.59996131888\n"
     ]
    }
   ],
   "source": [
    "J = 0\n",
    "w = np.zeros((n_x,C))\n",
    "b = np.zeros((1,C))\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "#Replace the None objects and alpha*0 by your code.\n",
    "\n",
    "# Convention: \n",
    "# Use capital letter when the variable is for the whole dataset of m train examples.\n",
    "\n",
    "for iter in range(10):\n",
    "    # X is (55000,784), Y is (55000,10), w is (784,10), b is (1,10)  \n",
    "    Z = X @ w + b              # Z is  (55000, 10)\n",
    "    A = softmax(Z)               # A is  (55000, 10)\n",
    "    J = -(1/m) * np.sum(Y * np.log(A))\n",
    "    print(iter, J)\n",
    "    dZ = A - Y                 # dZ is (55000, 10)\n",
    "    dw = (1/m) * X.T @ dZ              #dw is (784, 10) \n",
    "    db = (1/m) * np.sum(dZ,axis=0, keepdims=True)\n",
    "    w -= alpha*dw\n",
    "    b -= alpha*db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "0 2.30258509299\n",
    "1 2.19706377599\n",
    "2 2.10085951391\n",
    "3 2.0120441413\n",
    "4 1.92974131203\n",
    "5 1.85342090129\n",
    "6 1.78266241239\n",
    "7 1.71707943589\n",
    "8 1.65629942394\n",
    "9 1.59996131888\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6 - Compute the accuracy\n",
    "--\n",
    "Compute the accuracy of softmax classification on the train and test datasets.\n",
    "\n",
    "Use np.argmax(A, 1) and np.argmax(Y, 1) to find the predicted and real class for each example. They return an array of numbers each, e.g. [7 3 9 ..., 8 0 8] and [7 3 4 ..., 5 6 8]. Compare them using ==. You will get an array of booleans, e.g. [ True  True False ..., False False  True]. Sum up the latter using np.sum(). True values will be considered 1, False values will be considered 0, so the sum will tell us how many True values we got. Then divide by Y.shape[0] and multiply by 100 to get the accuracy in percentage.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  66.2927272727\n",
      "Accuracy on the test set is  67.05\n"
     ]
    }
   ],
   "source": [
    "# Replace None by your code\n",
    "\n",
    "def accuracy(A, Y):\n",
    "    return 100*np.sum(np.argmax(A,1) == np.argmax(Y,1)) / Y.shape[0]\n",
    "\n",
    "Z = X @ w + b\n",
    "A = softmax(Z)\n",
    "\n",
    "Z_test = X_test @ w + b\n",
    "A_test = softmax(Z_test)\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "Accuracy on the train set is  66.2927272727\n",
    "Accuracy on the test set is  67.05\n",
    "</pre>\n",
    "\n",
    "Remark. These numbers are for 10 iterations. When you perform 100 iterations, the numbers will be much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7 - Implement the Softmax classifier on TensorFlow\n",
    "--\n",
    "Implementing the Softmax classifier on TensorFlow is very similar to the TensorFlow example for binary logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the None objects with your code.\n",
    "\n",
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X)\n",
    "tf_Y = tf.constant(Y)\n",
    "tf_X_test = tf.constant(X_test)\n",
    "tf_Y_test = tf.constant(Y_test)\n",
    "\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable(tf.zeros((n_x,C)))\n",
    "tf_b = tf.Variable(tf.zeros((1,C)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inuts with the weight matrix, and add biases. \n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "\n",
    "# We compute\n",
    "# the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "# More concretely, set (uncomment)\n",
    "tf_J = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) ) \n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "# More concretely, set\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "\n",
    "# Uncomment the following lines. \n",
    "tf_A = tf.nn.softmax(tf_Z)\n",
    "tf_A_test = tf.nn.softmax(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 2.30271\n",
      "1 2.19706\n",
      "2 2.10086\n",
      "3 2.01204\n",
      "4 1.92974\n",
      "5 1.85342\n",
      "6 1.78266\n",
      "7 1.71708\n",
      "8 1.6563\n",
      "9 1.59996\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "# Replace None with your code\n",
    "\n",
    "for iter in range(10):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A]);    \n",
    "    # print the iteration number and the cost for that iteration\n",
    "    print(iter, J)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "Initialized\n",
    "0 2.30271\n",
    "1 2.19706\n",
    "2 2.10086\n",
    "3 2.01204\n",
    "4 1.92974\n",
    "5 1.85342\n",
    "6 1.78266\n",
    "7 1.71708\n",
    "8 1.6563\n",
    "9 1.59996\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is 77.5290909091\n",
      "Accuracy ont the test set is 79.02\n"
     ]
    }
   ],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A and tf_A_test.\n",
    "A=tf_A.eval()\n",
    "A_test=tf_A_test.eval()\n",
    "\n",
    "print( \"Accuracy on the train set is\", accuracy(A,Y))\n",
    "print( \"Accuracy ont the test set is\", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "Accuracy on the train set is  77.5290909091\n",
    "Accuracy on the test set is  79.02\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8 - Implement a neural network with one hidden layer in Tensorflow.\n",
    "--\n",
    "Turn the previous exercise into a 1-hidden layer neural network with rectified linear units and 15 hidden nodes. The output layer should continue to be softmax.\n",
    "\n",
    "You need to include another set of weights for the hidden layer. \n",
    "The hidden layer should be fully connected to the input layer and to the output layer. \n",
    "Use a learning rate of 0.5 and perform 300 iterations. \n",
    "\n",
    "To get a comparable accuracy with the simple softmax classifier, you should run at least 1001 iterations. Try it out if you have time (it will take some time to run). \n",
    "Print out the cost every 50 iterations.\n",
    "Depending on your machine, the training process could take several minutes.\n",
    "The cost should decrease in every iteration. If not, you have some bug in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the None objects with your code.\n",
    "\n",
    "num_hidden_nodes = 15\n",
    "\n",
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X)\n",
    "tf_Y = tf.constant(Y)\n",
    "tf_X_test = tf.constant(X_test)\n",
    "tf_Y_test = tf.constant(Y_test)\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training. The weight\n",
    "# matrices will be initialized using random values following a (truncated)\n",
    "# normal distribution. Use\n",
    "# tf.truncated_normal((first_dimension, second_dimension)) for that.\n",
    "# Specify the correct first_dimension and second_dimension for each weight matrix. \n",
    "# The biases get initialized to zero.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1,num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes,C]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1,C)))\n",
    "\n",
    "# Forward computation.\n",
    "tf_Z1 =tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.softmax(tf_Z2)\n",
    "\n",
    "tf_J = tf_J = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z2))\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.softmax(tf_Z2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 26.6508\n",
      "50 2.15463\n",
      "100 1.6895\n",
      "150 1.50505\n",
      "200 1.37959\n",
      "250 1.28086\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "\n",
    "# Replace None with your code.\n",
    "\n",
    "for iter in range(300):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    # Print out the iteration number and cost every 50 iterations.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2]);\n",
    "    \n",
    "    if iter%50==0 :\n",
    "        print(iter, J)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output*\n",
    "\n",
    "<pre>\n",
    "Initialized\n",
    "0 35.9117\n",
    "50 1.58068\n",
    "100 1.33825\n",
    "150 1.20126\n",
    "200 1.08693\n",
    "250 0.995857\n",
    "</pre>\n",
    "\n",
    "Remark. In this, and the following exercises, expect your results to be only approximately similar to the ones I put here. This is because we initialize the weights randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on trainning set is  61.8036363636\n",
      "Accuracy on test set is 62.54\n"
     ]
    }
   ],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A2 and tf_A2_test.\n",
    "A = tf_A2.eval()\n",
    "#print(A.shape[0])\n",
    "#print(Y.shape[0])\n",
    "A_test= tf_A2_test.eval()\n",
    "\n",
    "print(\"Accuracy on trainning set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on test set is\", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected result*\n",
    "\n",
    "<pre>\n",
    "Accuracy on the train set is  69.0272727273\n",
    "Accuracy on the test set is  69.15\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 9 - Stochastic Gradient Descent\n",
    "--\n",
    "As you observed above, 300 iterations are not enough to reach a good accuracy when using more layers, i.e. more weights (parameters) to train. We need more iterations and so it takes more time to train the network.\n",
    "\n",
    "Here we want to do batch stochastic gradient descent and approximate the gradient using batches of training examples. This will make training the network much faster, and we can train for many more iterations in a shorter time. \n",
    "\n",
    "Set num_steps = 5001 (number of iterations) and batch_size = 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the None objects by your code.\n",
    "\n",
    "num_hidden_nodes = 15\n",
    "\n",
    "# Input data.\n",
    "# Let's use placeholders for the training data. \n",
    "# This is so that we can suply batches of tranining examples each iteration.\n",
    "tf_X = tf.placeholder(tf.float32)\n",
    "tf_Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training. The weight\n",
    "# matrices will be initialized using random values following a (truncated)\n",
    "# normal distribution. The biases get initialized to zero.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1,num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes,C]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1,C)))\n",
    "\n",
    "# Forward computation.\n",
    "tf_Z1 =tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.softmax(tf_Z2)\n",
    "\n",
    "tf_J = tf_J = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z2))\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.softmax(tf_Z2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step  (0, 23.631908)\n",
      "Minibatch accuracy:  17.0\n",
      "Test accuracy:  11.33\n",
      "Minibatch loss at step  (500, 1.048203)\n",
      "Minibatch accuracy:  68.0\n",
      "Test accuracy:  66.89\n",
      "Minibatch loss at step  (1000, 0.8027823)\n",
      "Minibatch accuracy:  75.0\n",
      "Test accuracy:  72.56\n",
      "Minibatch loss at step  (1500, 0.57790101)\n",
      "Minibatch accuracy:  77.0\n",
      "Test accuracy:  77.5\n",
      "Minibatch loss at step  (2000, 0.5561524)\n",
      "Minibatch accuracy:  83.0\n",
      "Test accuracy:  81.57\n",
      "Minibatch loss at step  (2500, 0.61692917)\n",
      "Minibatch accuracy:  80.0\n",
      "Test accuracy:  82.02\n",
      "Minibatch loss at step  (3000, 0.54180115)\n",
      "Minibatch accuracy:  81.0\n",
      "Test accuracy:  84.24\n",
      "Minibatch loss at step  (3500, 0.35670322)\n",
      "Minibatch accuracy:  87.0\n",
      "Test accuracy:  85.17\n",
      "Minibatch loss at step  (4000, 0.63068652)\n",
      "Minibatch accuracy:  85.0\n",
      "Test accuracy:  84.87\n",
      "Minibatch loss at step  (4500, 0.68121642)\n",
      "Minibatch accuracy:  80.0\n",
      "Test accuracy:  86.18\n",
      "Minibatch loss at step  (5000, 0.28934476)\n",
      "Minibatch accuracy:  91.0\n",
      "Test accuracy:  87.07\n"
     ]
    }
   ],
   "source": [
    "# Replace the None objects by your code.\n",
    "\n",
    "num_steps = 5001\n",
    "batch_size = 100\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data.\n",
    "    offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch of size batch_size.\n",
    "    X_batch = X[offset:(offset + batch_size), :]\n",
    "    Y_batch = Y[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Uncomment the following lines once you complete the above\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2], feed_dict={tf_X : X_batch, tf_Y : Y_batch})\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step \", (step, J))\n",
    "        print(\"Minibatch accuracy: \", accuracy(A, Y_batch))\n",
    "        A_test = tf_A2_test.eval()\n",
    "        print(\"Test accuracy: \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected result*\n",
    "\n",
    "<pre>\n",
    "Initialized\n",
    "Minibatch loss at step  (0, 28.198519)\n",
    "Minibatch accuracy:  8.0\n",
    "Test accuracy:  14.12\n",
    "Minibatch loss at step  (500, 0.65256858)\n",
    "Minibatch accuracy:  79.0\n",
    "Test accuracy:  76.83\n",
    "Minibatch loss at step  (1000, 1.3163916)\n",
    "Minibatch accuracy:  76.0\n",
    "Test accuracy:  81.71\n",
    "Minibatch loss at step  (1500, 0.38738632)\n",
    "Minibatch accuracy:  87.0\n",
    "Test accuracy:  87.19\n",
    "Minibatch loss at step  (2000, 0.33017638)\n",
    "Minibatch accuracy:  91.0\n",
    "Test accuracy:  87.71\n",
    "Minibatch loss at step  (2500, 0.45241368)\n",
    "Minibatch accuracy:  85.0\n",
    "Test accuracy:  88.78\n",
    "Minibatch loss at step  (3000, 0.2176722)\n",
    "Minibatch accuracy:  96.0\n",
    "Test accuracy:  90.1\n",
    "Minibatch loss at step  (3500, 0.32031158)\n",
    "Minibatch accuracy:  89.0\n",
    "Test accuracy:  89.45\n",
    "Minibatch loss at step  (4000, 0.54982322)\n",
    "Minibatch accuracy:  89.0\n",
    "Test accuracy:  88.64\n",
    "Minibatch loss at step  (4500, 0.51812667)\n",
    "Minibatch accuracy:  85.0\n",
    "Test accuracy:  90.7\n",
    "Minibatch loss at step  (5000, 0.20552643)\n",
    "Minibatch accuracy:  95.0\n",
    "Test accuracy:  91.01\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just for curiosity, make the number of hidden units above quite larger, e.g. 1000, and run the training again. Yes, we can do that using batch SGD and be able to train the quite large network in a reasonable time (several minutes).\n",
    "\n",
    "What's the accuracy you get? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
